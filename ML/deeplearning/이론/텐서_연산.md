#  벡터와 행렬 연산, tensor

[딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/book/2155) 내용을 정리했습니다.

## 학습 목표

* 소프트맥스 회귀에서종속 변수 y의 종류가 3개 이상이 되면서 복잡해진다. 이러한 식들이 겹겹이 누적되면 ANN이 된다. 케라스 API 는 사용하기 편리해서 이런 고민을 할 일이 상대적으로 적지만, Numpy나 텐서플로우의 로우레벨 머신러닝 개발을 하게 되면 각 변수의 연산을 벡터와 행렬 연산으로 이해할 수 있어야 함. 

* 즉 사용자가 데이터와 변수의 개수로부터 행렬의 크기 더 나아가 텐서의 크기를 산정할 수 있어야 한다.

  

## 0. 벡터와 행렬과 텐서

* 벡터는 크기와 방향을 가진 양, 파이썬에서는 리스트로 표현
* 행렬은 2차원 형상의 구조, 파이썬에서는 2차원 배열로 표현(row, column)
* 3차원부터 주로 텐서라고 부름.



## 1. 텐서(Tensor)

* ANN 은 복잡한 모델 내의 연산을 주로 행렬 연산을 통해 해결함. 여기서 말하는 행렬 연산이란 단순히 2차원 배열을 통한 것만이 아님. 머신러닝의 입, 출력이 복잡해지면(예컨대 RNN) 텐서에 대한 이해가 필수로 요구됨.

* 텐서를 설명하기 위한 아래의 코드는 Numpy를 임포트 했다고 가정함.

  ```python
  import numpy as np
  ```

  

### 1) 0차원 텐서

* 하나의 실수 값으로 이루어진 데이터, 스칼라값을 0차원 텐서라고 함.

  ```python
  d = np.array(5)
  print(d.ndim) # 차원 수 출력
  print(d.shape) # 텐서의 크기 출력
  
  # 0
  # ()
  ```

  ```python
  0
  ()	# 크기가 없음
  ```

  



### 2) 1차원 텐서

* 벡터: 숫자를 특정 순서대로 배열한 것을 의미

* 벡터의 차원과 텐서의 차원은 다른 개념임. 아래 예제는 4차원 백터지만 1차원 텐서임. 1D 텐서라고도 함

  ```python
  d = np.array([1, 2, 3, 4])
  print(d.ndim)
  print(d.shape)
  ```

  ```python
  1
  (4,)
  ```

  * 벡터에서 차원은 하나의 축에 차원들이 존재하는 것이고 텐서에서의 차원은 축의 개수를 의미함



### 3) 2차원 텐서

* 행과 열이 존재하는 벡터의 배열, 즉 행렬을 2차원 텐서라고 한다. 2D 텐서라고도 한다.

  ```python
  d = np.array([1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12])
  print(d.ndim)
  print(d.shape)
  ```

  ```
  2
  (3, 4)
  ```

  * 텐서의 크기를 보고 머리 속에 떠올릴 수 있으면 모델 설계 시에 유용함. 이게 어렵다면, 큰 단위부터 확장해나가며 생각하면 됨. 
  * 위의 경우 3개의 커다란 데이터가 있는데, 그 각각의 커다란 데이터는 작은 데이터 4개로 이루어졌다고 생각할 수 있음.
  * 1차원 텐서를 벡터, 2차원 텐서를 행렬에 비유하는데, 이를 선형대수에서 행렬의 열을 열벡터로 부르는 것과 혼동하면 안 됨. 1차원 텐서와 2차원 텐서는 차원 자체가 달라야 함.



### 4) 3차원 텐서

* 행렬 또는 2차원 텐서를 단위로 한 번 더 배열하면 3차원 텐서라고 부름. 3D 텐서라고도 함. 사실 위에서 언급한 0 ~ 2차원  텐서는 각각 스칼라, 벡터, 행렬이라고 해도 무방하므로, 3차원 이상의 텐서부터 본격적으로 텐서라고 부름. 

* 이 3차원 텐서의 구조를 이해하지 않으면 복잡한 ANN의 입, 출력 값을 이해하는 것이 어려움. 개념 자체는 어렵지 않으나 반드시 알아야 함.

  ```python
  d = np.array([
    [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15]],
    [[16, 17, 18, 19, 20], [21, 22, 23, 24, 25], [26, 27, 28, 29, 30]]
  ])
  
  print(d.ndim)
  print(d.shape)
  ```

  ```python
  3
  (2, 3, 5)
  ```

  * 2개의 큰 데이터가 있는데, 각각은 3개의 더 작은 데이터로 구성되며, 그 3개의 데이터는 또한 더 작은 5개의 데이터로 구성됨

    

* 자연어 처리에서 자주 보게 되는 것이 이 3D 텐서임. 3D 텐서는 시퀸스 데이터를 표현할 때 자주 사용됨. 여기서 시퀸스 데이터는 주로 단어의 시퀸스를 의미하며 시퀸스는 주로 문장이나 문서, 뉴스 기사 등의 텍스트가 될 수 있음. 

* 이 경우 3D 텐서는 (samples, timesteps, word_dim) 이 됨. 또는 일괄로 처리하기 위해 데이터를 묶는 단위인 배치의 개념을 따라 (batch_size, timesteps, word_dim) 이라고도 할 수 있음

* sample/batch_size는 데이터의 개수, timesteps는 시퀸스의 길이, word_dim은 단어를 표현하는 벡터의 차원을 의미함.

* 왜 nlp에서 3D 텐서가 쓰이는가? 

  * 다음과 같은 훈련 데이터가 있다고 가정하자.

    * 문서1: I like NLP
    * 문서2: I like DL
    * 문서3: DL is AI

  * 이를 ANN 모델의 입력으로 사용하기 위해서 각 단어를 벡터화 해야 함. 원-핫 인코딩이나 워드 임베딩 등으로. 원-핫 인코딩으로 벡터화 해보자.

    | 단어 | One-hot vector |
    | :--- | :------------- |
    | I    | [1 0 0 0 0 0]  |
    | like | [0 1 0 0 0 0]  |
    | NLP  | [0 0 1 0 0 0]  |
    | DL   | [0 0 0 1 0 0]  |
    | is   | [0 0 0 0 1 0]  |
    | AI   | [0 0 0 0 0 1]  |

  * 기존의 훈련 데이터를 모두 바꿔서 ANN의 입력을 사용한다고 하면 다음과 같음. (이렇게 후녈ㄴ 데이터를 여러개 묶어서 한 번에 입력으로 사용하는 것을 배치라고 함)
    * [[[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0]],
      [[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]],
      [[0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]]]
  * 이는 `(3, 3, 6)`의 크기를 가지는 3D 텐서임



### 5) 그 이상의 텐서

* 3차원 텐서를 배열로 합치면 4차원 텐서가 됨. 4차원 텐서를 배열로 합치면 5차원 텐서가 됨. 이런 식으로 텐서는 배열로서 계속해서 확장될 수 있음.

<img src="https://user-images.githubusercontent.com/46865281/77846498-7c9de100-71f1-11ea-8701-e90ca58ce9de.png" alt="image" style="zoom:80%;" />



## 2. 벡터와 행렬의 연산

### 1) 덧셈과 뺄셈

* 벡터의 경우

  <img src="https://user-images.githubusercontent.com/46865281/77846579-f8982900-71f1-11ea-9818-a2258d0be6ea.png" alt="image" style="zoom:50%;" />

  ```python
  a = np.array([8, 4, 5])
  b = np.array([1, 2, 3])
  print(a+b)
  print(a-b)
  ```

  ```python
  [9 6 8]
  [7 2 2]
  ```

  

* 행렬의 경우

  <img src="https://user-images.githubusercontent.com/46865281/77846615-414fe200-71f2-11ea-82c9-532fc5880628.png" alt="image" style="zoom:50%;" />

  ```python
  a = np.array([[10, 20, 30, 40], [50, 60, 70, 80]])
  b = np.array([[5, 6, 7, 8],[1, 2, 3, 4]])
  print(a+b)
  print(a-b)
  ```

  ```python
  [[15 26 37 48]
   [51 62 73 84]]
  [[ 5 14 23 32]
   [49 58 67 76]]
  ```

  

### 2) 내적과 곱셈

* 벡터 점곱(dot product) = 내적(inner product)의 경우

  * 내적은 두 벡터의 차원이 같아야 하며, 앞의 벡터가 행벡터, 뒤 벡터가 열벡터여야 함

  * 벡터 내적의 결과는 스칼라가 됨

    <img src="https://user-images.githubusercontent.com/46865281/77846691-c9ce8280-71f2-11ea-9285-401553cd4485.png" alt="image" style="zoom:50%;" />

    ```python
    a = np.array([1, 2, 3])
    b = np.array([4, 5, 6])
    print(np.dot(a, b))
    ```

    ```python
    32
    ```

    

* 행렬 곱셈의 경우

  <img src="/Users/seungyoungoh/Library/Application Support/typora-user-images/image-20200329192947527.png" alt="image-20200329192947527" style="zoom:50%;" />
  ```python
  a = np.array([1, 3], [2, 4])
  b = np.array([5, 7], [6, 8])
  print(np.matmul(a,b))
  ```

  ```python
  [[23 31]
   [34 46]]
  ```





## 3. 다중 선형회귀 행렬 연산으로 이해하기

* 독립 변수가 2개 이상일 때, 1개의 종속 변수를 예측하는 문제를 행렬의 연산으로 표현한다면 어떻게 될까? 다중 선형 회귀나 다중 로지스틱 회귀가 이러한 연산의 예임.

* <img src="https://user-images.githubusercontent.com/46865281/77846925-57f73880-71f4-11ea-89d7-32fc6047bad5.png" alt="image" style="zoom:50%;" />

  위의 다중 선형회귀 식은 입력 벡터와 가중치 벡터의 내적으로 효현될 수 있음

  <img src="https://user-images.githubusercontent.com/46865281/77846939-70675300-71f4-11ea-977f-4f879a41a370.png" alt="image" style="zoom:50%;" />

* 데이터의 개수가 많은 경우, 벡터의 내적이 아니라 행렬의 곱셈으로 표현함. 

  * 다음 데이터를 다중 회귀분석 한다고 가정하자. 예측하려는 종속 변수는 집의 가격이다.

    | size(feet2)(x1) | number of bedrooms(x2) | number of floors(x3) | age of home(x4) | price($1000)(y) |
    | :-------------- | :--------------------- | :------------------- | :-------------- | :-------------- |
    | 1800            | 2                      | 1                    | 10              | 207             |
    | 1200            | 4                      | 2                    | 20              | 176             |
    | 1700            | 3                      | 2                    | 15              | 213             |
    | 1500            | 5                      | 1                    | 10              | 234             |
    | 1100            | 2                      | 2                    | 10              | 155             |

  * 위 데이터를 입력 벡터 **X**와 출력 벡터 **W**의 곱으로 표현하면 다음과 같음

    <img src="https://user-images.githubusercontent.com/46865281/77847026-f5526c80-71f4-11ea-94d2-60d86dc17db1.png" alt="image" style="zoom:50%;" />

  * 여기에 편향 벡터 **B**를 더해주면 전체 가설 수식 **H(X)**를 표현할 수 있음

    <img src="https://user-images.githubusercontent.com/46865281/77847063-2df24600-71f5-11ea-9b5d-e6f182827b35.png" alt="image" style="zoom:50%;" />

    <img src="https://user-images.githubusercontent.com/46865281/77847092-685be300-71f5-11ea-8593-b57b0e6697bf.png" alt="image" style="zoom:50%;" />

